{"cells":[{"cell_type":"markdown","source":[""],"metadata":{"id":"AAaRaVRY44Nd"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GwAaeaTb-CST","executionInfo":{"status":"ok","timestamp":1647491165929,"user_tz":-210,"elapsed":2769,"user":{"displayName":"M. dehghani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3pIlEuIhuA0QP4lXlo_3iiNupJy0O2ICOtw_5=s64","userId":"17974684348231269790"}},"outputId":"e379fd3f-1aa8-4c58-a4fc-7fe1d4b74d63"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ta5-6HragsaK"},"outputs":[],"source":["!pip install ninja\n","!pip install wandb"]},{"cell_type":"code","source":["import wandb"],"metadata":{"id":"a1KlSaagfsen","executionInfo":{"status":"ok","timestamp":1647491174938,"user_tz":-210,"elapsed":60,"user":{"displayName":"M. dehghani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3pIlEuIhuA0QP4lXlo_3iiNupJy0O2ICOtw_5=s64","userId":"17974684348231269790"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["wandb.init(project=\"Self-Correction-Human-Parsing-For_Binary-Images\")"],"metadata":{"id":"i0TznHI9fB5v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = wandb.config\n","config.learning_rate = 0.001"],"metadata":{"id":"ltg-4GCW7AP7","executionInfo":{"status":"ok","timestamp":1647491184886,"user_tz":-210,"elapsed":107,"user":{"displayName":"M. dehghani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3pIlEuIhuA0QP4lXlo_3iiNupJy0O2ICOtw_5=s64","userId":"17974684348231269790"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":108,"status":"ok","timestamp":1647491184895,"user":{"displayName":"M. dehghani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3pIlEuIhuA0QP4lXlo_3iiNupJy0O2ICOtw_5=s64","userId":"17974684348231269790"},"user_tz":-210},"id":"ZdS_goiSVQlC","outputId":"1eb8c00b-93a0-4516-aa65-d526d29bbb34"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing\n"]}],"source":["%cd /content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_HkQN_pvVraL"},"outputs":[],"source":["!mkdir train_image\n","!mkdir train_segmantation\n","!mkdir val_images\n","!mkdir val_segmantation"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3404,"status":"ok","timestamp":1647491188199,"user":{"displayName":"M. dehghani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3pIlEuIhuA0QP4lXlo_3iiNupJy0O2ICOtw_5=s64","userId":"17974684348231269790"},"user_tz":-210},"id":"536sM0qqfSKX","outputId":"12b86034-24a5-49b7-9a32-34b89e4bc5b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python==4.4.0.46 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (4.4.0.46)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==4.4.0.46->-r requirements.txt (line 1)) (1.21.5)\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"-sW1oWViWTUF","executionInfo":{"status":"ok","timestamp":1647491188201,"user_tz":-210,"elapsed":70,"user":{"displayName":"M. dehghani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3pIlEuIhuA0QP4lXlo_3iiNupJy0O2ICOtw_5=s64","userId":"17974684348231269790"}}},"outputs":[],"source":["import gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZEOZKB10Xf5A"},"outputs":[],"source":["url = \"https://drive.google.com/uc?id=1E5YwNKW2VOEayK9mWCS3Kpsxf-3z04ZE\"\n","out = \"weights/exp-schp-201908270938-pascal-person-part.pth\"\n","gdown.download(url,out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W78Qq02OZQ71"},"outputs":[],"source":["!python simple_extractor.py --dataset 'pascal' --model-restore \"weights/exp-schp-201908270938-pascal-person-part.pth\" --input-dir \"./input/JPEGImages\" --output-dir \"./output\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gLRAmRfMX1G2"},"outputs":[],"source":["import os\n","import cv2\n","\n","path = \"/content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing/output\"\n","list_files = os.listdir(path)[0:1000]\n","f1 =  open('/content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing/dataset/train_id.txt', 'w')\n","f2 = open('/content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing/dataset/val_id.txt', 'w')\n","\n","for i, im in enumerate(list_files):\n","  img = cv2.imread(os.path.join(path, im))\n","  img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","  img_gray[img_gray > 0] = 255\n","  img_gray[img_gray == 0] = 0       \n","  # print(im)\n","  _, tail = os.path.split(im)\n","  # # print(tail)\n","  name = tail.split('.')[0]\n","  name = name.split('_')[1]\n","  # print(name)\n","\n","  if i < 800:\n","    cv2.imwrite(f\"/content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing/dataset/train_images/{name}.jpg\", img_gray)\n","    f1.write(f\"{name}\")\n","    f1.write('\\n')\n","\n","  else:\n","    cv2.imwrite(f\"/content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing/dataset/val_images/{name}.jpg\", img_gray)\n","    f2.write(f\"{name}\")\n","    f2.write('\\n')\n","\n","  \n","f1.close()\n","f2.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HZQNYw5kZMn2"},"outputs":[],"source":["# Create labels\n","\n","import os\n","import cv2\n","import numpy as np\n","\n","path = \"/content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing/output\"\n","list_files = os.listdir(path)[0:1000]\n","\n","\n","for i, im in enumerate(list_files):\n","  img_rgb = cv2.imread(os.path.join(path, im))\n","  img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n","  # img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","  img_gray[img_gray == 0] = 0\n","  img_gray[img_gray == 15] = 1\n","  img_gray[img_gray == 38] = 2\n","  img_gray[img_gray == 53] = 3\n","  img_gray[img_gray == 75] = 4\n","  img_gray[img_gray == 90] = 5\n","  img_gray[img_gray == 113] = 6\n","  # print(im)\n","  _, tail = os.path.split(im)\n","  name = tail.split('.')[0]\n","  name = name.split('_')[1]\n","  \n","  \n","\n","  if i < 800:\n","    cv2.imwrite(f\"/content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing/dataset/train_segmentations/{name}.png\", img_gray)\n","  else:\n","    cv2.imwrite(f\"/content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing/dataset/val_segmentations/{name}.png\", img_gray)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2CGSgOCZ4AN"},"outputs":[],"source":["# visualization\n","\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","fig = plt.figure(figsize=(10, 10))\n","\n","img = cv2.imread(\"/content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing/input/JPEGImages/2500_91.jpg\")\n","img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","plt.subplot(141) \n","plt.imshow(img)\n","\n","img1 = cv2.imread(\"/content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing/output/2500_91.png\")\n","plt.subplot(142) \n","plt.imshow(img1)\n","\n","img = cv2.imread(\"/content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing/dataset/val_images/91.jpg\")\n","plt.subplot(143) \n","plt.imshow(img, cmap='gray')\n","\n","img = cv2.imread(\"/content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing/dataset/val_segmentations/91.png\")\n","plt.subplot(144) \n","plt.imshow(img, cmap='gray')"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"zCAQuVOPc2xb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c9e8de7b-6dec-4927-a848-45cb51a0dac4","executionInfo":{"status":"ok","timestamp":1647491159714,"user_tz":-210,"elapsed":373386,"user":{"displayName":"M. dehghani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3pIlEuIhuA0QP4lXlo_3iiNupJy0O2ICOtw_5=s64","userId":"17974684348231269790"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcyber\u001b[0m (use `wandb login --relogin` to force relogin)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing/wandb/run-20220317_070944-37tdbpwf\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mglowing-forest-25\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cyber/Face_Detection\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cyber/Face_Detection/runs/37tdbpwf\u001b[0m\n","Namespace(arch='resnet101', batch_size=3, cycle_epochs=10, data_dir='dataset', epochs=20, eval_epochs=10, gpu='0,1,2', ignore_label=255, imagenet_pretrain='./pretrain_model/resnet101-imagenet.pth', input_size='473,473', lambda_c=0.1, lambda_e=1, lambda_s=1, learning_rate=0.001, log_dir='./log', model_restore='./log/checkpoint.pth.tar', momentum=0.9, num_classes=7, random_mirror=False, random_scale=False, schp_restore='./log/schp_checkpoint.pth.tar', schp_start=100, start_epoch=0, weight_decay=0.0005)\n","image mean: [0.406, 0.456, 0.485]\n","image std: [0.225, 0.224, 0.229]\n","input space:BGR\n","BGR Transformation\n","Total training samples: 800\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","iter = 0 of 1760 completed, lr = 1e-05, loss = 3.749821424484253\n","epoch = 0 of 20 completed using 219.95830373900003 s\n","iter = 100 of 1760 completed, lr = 0.00010899999999999999, loss = 3.184112310409546\n","epoch = 1 of 20 completed using 176.03309954899999 s\n","iter = 200 of 1760 completed, lr = 0.000208, loss = 1.6987113952636719\n","epoch = 2 of 20 completed using 161.55483840366665 s\n","iter = 300 of 1760 completed, lr = 0.00030700000000000004, loss = 1.143749475479126\n","epoch = 3 of 20 completed using 154.32424390650004 s\n","iter = 400 of 1760 completed, lr = 0.000406, loss = 1.1187372207641602\n","epoch = 4 of 20 completed using 150.01277456780002 s\n","iter = 500 of 1760 completed, lr = 0.000505, loss = 0.7733674645423889\n","epoch = 5 of 20 completed using 147.26431601883337 s\n","iter = 600 of 1760 completed, lr = 0.000604, loss = 0.7792283296585083\n","epoch = 6 of 20 completed using 145.17961165100002 s\n","iter = 700 of 1760 completed, lr = 0.0007030000000000001, loss = 0.536536693572998\n","epoch = 7 of 20 completed using 143.6391407625 s\n","epoch = 8 of 20 completed using 142.42329932111113 s\n","iter = 800 of 1760 completed, lr = 0.000901, loss = 0.6024234294891357\n","\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ReadTimeout), entering retry loop.\n","epoch = 9 of 20 completed using 141.5472592342 s\n","iter = 900 of 1760 completed, lr = 0.001, loss = 0.4847472608089447\n","epoch = 10 of 20 completed using 140.80592869972727 s\n","iter = 1000 of 1760 completed, lr = 0.0009996984593744526, loss = 0.44234344363212585\n","epoch = 11 of 20 completed using 140.12510538725 s\n","iter = 1100 of 1760 completed, lr = 0.000998794204878613, loss = 0.4837489426136017\n","epoch = 12 of 20 completed using 139.54775524653846 s\n","iter = 1200 of 1760 completed, lr = 0.0009972883382072953, loss = 0.5863214135169983\n","epoch = 13 of 20 completed using 139.0599673952857 s\n","iter = 1300 of 1760 completed, lr = 0.0009951826940270774, loss = 0.4678802490234375\n","epoch = 14 of 20 completed using 138.64498106180002 s\n","iter = 1400 of 1760 completed, lr = 0.000992479837741043, loss = 0.439550518989563\n","epoch = 15 of 20 completed using 138.270714614 s\n","epoch = 16 of 20 completed using 137.9435780635294 s\n","iter = 1500 of 1760 completed, lr = 0.0009852963845066183, loss = 0.45309776067733765\n","epoch = 17 of 20 completed using 137.64157840483333 s\n","iter = 1600 of 1760 completed, lr = 0.0009808245394894678, loss = 0.4586260914802551\n","epoch = 18 of 20 completed using 137.38400463152632 s\n","iter = 1700 of 1760 completed, lr = 0.0009757729755661011, loss = 0.40690553188323975\n","epoch = 19 of 20 completed using 137.19482679380002 s\n","Training Finished in 2743.89713404 seconds\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m: Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:  Loss █▅▃▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m: Epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:  Loss 0.47115\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mglowing-forest-25\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/cyber/Face_Detection/runs/37tdbpwf\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220317_070944-37tdbpwf/logs\u001b[0m\n"]}],"source":["%cd /content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing\n","!python train.py --data-dir dataset --num-classes 7 --batch-size 3 --imagenet-pretrain ./pretrain_model/resnet101-imagenet.pth"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdbxThRtdLPy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"96866a1f-670c-4a05-9b73-1a6ad6b3527a"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing\n","Evaluating total class number 7 with ['Background', 'Head', 'Torso', 'Upper Arms', 'Lower Arms', 'Upper Legs', 'Lower Legs']\n"," 98% 775/787 [05:08<00:04,  2.45it/s]"]}],"source":["%cd /content/drive/MyDrive/Human_parsing/Self-Correction-Human-Parsing\n","!python simple_extractor.py --dataset 'pascal' --model-restore \"./log/checkpoint_20.pth.tar\" --input-dir './dataset/train_images' --output-dir './binary_output'"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Human_Parsing.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}